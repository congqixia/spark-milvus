package com.zilliz.spark.connector.operations.backfill

import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.util.CaseInsensitiveStringMap
import org.apache.spark.sql.connector.write.DataWriter
import org.slf4j.LoggerFactory

import com.zilliz.spark.connector.write.{MilvusLoonBatchWrite, MilvusLoonCommitMessage, MilvusLoonWriter}
import com.zilliz.spark.connector.{MilvusClient, MilvusConnectionParams, MilvusOption}
import com.zilliz.spark.connector.read.{MilvusSnapshotReader, SnapshotMetadata, StorageV2ManifestItem}

import org.apache.hadoop.fs.Path
import scala.collection.JavaConverters._


/**
 * Backfill operation for Milvus collections
 *
 * This object provides functionality to backfill new fields into existing Milvus collections
 * by reading the original data, joining with new field data, and writing per-segment binlog files.
 */
object MilvusBackfill {

  private val logger = LoggerFactory.getLogger(getClass)

  /**
   * Backfill new fields into a Milvus collection
   *
   * @param spark SparkSession
   * @param backfillDataPath Path to Parquet file containing new field data with schema (pk, new_field1, new_field2, ...)
   * @param snapshotPath Path to Milvus snapshot metadata JSON file
   * @param config Backfill configuration
   * @return Either error or successful result
   */
  def run(
      spark: SparkSession,
      backfillDataPath: String,
      snapshotPath: String,
      config: BackfillConfig
  ): Either[BackfillError, BackfillResult] = {

    val startTime = System.currentTimeMillis()

    // Validate S3/writer configuration (always required)
    config.validate() match {
      case Left(error) => return Left(SchemaValidationError(s"Invalid configuration: $error"))
      case Right(_) => // Continue
    }

    // Step 1: Try to load snapshot metadata
    val snapshotMetadataOpt = loadSnapshotMetadata(spark, snapshotPath, config)

    // Step 2: Create Milvus client only if no snapshot is available
    var client: MilvusClient = null
    if (snapshotMetadataOpt.isEmpty) {
      config.validateForClientMode() match {
        case Left(error) => return Left(SchemaValidationError(
          s"No snapshot provided and invalid client configuration: $error"))
        case Right(_) =>
      }
      client = MilvusClient(
        MilvusConnectionParams(
          uri = config.milvusUri,
          token = config.milvusToken,
          databaseName = config.databaseName
        )
      )
    }

    try {
      // Step 3: Get PK field info
      val (pkName, pkFieldId) = snapshotMetadataOpt match {
        case Some(metadata) =>
          val pkField = metadata.collection.schema.fields.find(_.isPrimaryKey.getOrElse(false))
            .getOrElse(return Left(SchemaValidationError("No primary key field found in snapshot schema")))
          (pkField.name, pkField.getFieldIDAsLong)
        case None =>
          client.getPkField(config.databaseName, config.collectionName) match {
            case scala.util.Success((name, id)) => (name, id)
            case scala.util.Failure(e) => return Left(ConnectionError(
              message = s"Failed to get PK field: ${e.getMessage}", cause = Some(e)))
          }
      }

      // Read backfill data from Parquet
      val backfillDF = readBackfillData(spark, backfillDataPath) match {
        case Left(error) => return Left(error)
        case Right(df) => df
      }

      // Read original collection data with segment metadata
      val originalDF = readCollectionWithMetadata(spark, config, pkFieldId, snapshotMetadataOpt) match {
        case Left(error) => return Left(error)
        case Right(df) => df
      }

      // Validate schema compatibility
      validateSchemaCompatibility(originalDF, backfillDF, pkName) match {
        case Left(error) => return Left(error)
        case Right(_) => // Continue
      }

      // Perform Sort Merge Join
      val joinedDF = performJoin(originalDF, backfillDF, pkName)

      // Step 4: Get collection metadata (collectionID, segment-to-partition mapping, base paths)
      val (collectionID, segmentToPartitionMap, segmentBasePathMap) = snapshotMetadataOpt match {
        case Some(metadata) =>
          extractMetadataFromSnapshot(metadata)
        case None =>
          val (colId, segPartMap) = retrieveMilvusMetadata(config, client) match {
            case Left(error) => return Left(error)
            case Right(metadata) => metadata
          }
          (colId, segPartMap, Map.empty[Long, String])
      }

      // Extract new field names
      val newFieldNames = backfillDF.schema.fields
        .map(_.name)
        .filterNot(_ == "pk")
        .toSeq

      // Build field name -> field ID mapping from collection schema
      val fieldNameToId: Map[String, Long] = snapshotMetadataOpt match {
        case Some(metadata) =>
          MilvusSnapshotReader.getFieldNameToIdMap(metadata.collection.schema)
        case None =>
          // TODO: get from Milvus client when needed
          Map.empty
      }

      // Filter to only the new fields being backfilled
      val newFieldNameToId = newFieldNames.flatMap(n => fieldNameToId.get(n).map(n -> _)).toMap

      // Process each segment
      val segmentResults = processSegments(
        spark,
        joinedDF,
        collectionID,
        segmentToPartitionMap,
        segmentBasePathMap,
        config,
        newFieldNames,
        newFieldNameToId
      ) match {
        case Left(error) => return Left(error)
        case Right(results) => results
      }

      // Build final result
      val executionTime = System.currentTimeMillis() - startTime
      val partitionIDs = segmentToPartitionMap.values.toSet

      val result = BackfillResult.success(
        segmentResults = segmentResults,
        executionTimeMs = executionTime,
        collectionId = collectionID,
        partitionId = if (partitionIDs.size == 1) partitionIDs.head else -1,
        newFieldNames = newFieldNames
      )

      Right(result)

    } catch {
      case e: Exception =>
        val executionTime = System.currentTimeMillis() - startTime
        logger.error("Backfill operation failed", e)
        Left(BackfillError.fromException(e))
    } finally {
      if (client != null) {
        try {
          client.close()
        } catch {
          case e: Exception =>
            logger.warn("Failed to close Milvus client", e)
        }
      }
    }
  }

  /**
   * Read backfill data from Parquet file
   */
  private def readBackfillData(
      spark: SparkSession,
      path: String
  ): Either[BackfillError, DataFrame] = {
    try {
      val df = spark.read.parquet(path)

      // Validate that it has a 'pk' column
      if (!df.columns.contains("pk")) {
        return Left(DataReadError(
          path = path,
          message = "Backfill data must contain a 'pk' column"
        ))
      }

      // Validate that it has at least one other column
      if (df.columns.length < 2) {
        return Left(DataReadError(
          path = path,
          message = "New field data must contain at least one field besides 'pk'"
        ))
      }

      Right(df)
    } catch {
      case e: Exception =>
        logger.error(s"Failed to read Parquet file from $path", e)
        Left(DataReadError(
          path = path,
          message = s"Failed to read Parquet file: ${e.getMessage}",
          cause = Some(e)
        ))
    }
  }

  /**
   * Read collection data with segment_id and row_offset metadata
   * segment_id and row_offset are used to match with the original sequence of rows for each segment
   *
   * @param pkFieldId Primary key field ID to read only PK field
   * @param snapshotMetadata Optional snapshot metadata for offline reading (no client connection)
   */
  private def readCollectionWithMetadata(
      spark: SparkSession,
      config: BackfillConfig,
      pkFieldId: Long,
      snapshotMetadata: Option[SnapshotMetadata]
  ): Either[BackfillError, DataFrame] = {
    try {
      var options = config.getMilvusReadOptions
      options = options + (MilvusOption.ReaderFieldIDs -> pkFieldId.toString)

      // If snapshot metadata is available, use snapshot-based reading (no client calls)
      snapshotMetadata.foreach { metadata =>
        // Enable snapshot mode flag
        options = options + (MilvusOption.SnapshotMode -> "true")

        // Override connection options for snapshot mode (no client needed)
        options = options + ("milvus.uri" -> "dummy://snapshot-mode")
        options = options + ("milvus.collection.name" -> metadata.collection.schema.name)

        // Add snapshot collection ID
        options = options + (MilvusOption.SnapshotCollectionId -> metadata.snapshotInfo.collectionId.toString)

        // Add snapshot partition IDs
        options = options + (MilvusOption.SnapshotPartitionIds -> metadata.snapshotInfo.partitionIds.mkString(","))

        // Convert snapshot schema to protobuf bytes and pass as Base64
        val schemaBytes = MilvusSnapshotReader.toProtobufSchemaBytes(metadata.collection.schema)
        val schemaBytesBase64 = java.util.Base64.getEncoder.encodeToString(schemaBytes)
        options = options + (MilvusOption.SnapshotSchemaBytes -> schemaBytesBase64)

        metadata.storageV2ManifestList.foreach { manifestList =>
          // Pass original manifest JSON (containing both ver and base_path) so that
          // the DataSource can extract readVersion and lock reads to snapshot version
          if (manifestList.nonEmpty) {
            val manifestJson = MilvusSnapshotReader.serializeManifestList(manifestList)
            options = options + (MilvusOption.SnapshotManifests -> manifestJson)
          } else {
            logger.warn("No valid manifests found in snapshot")
          }
        }
      }

      // Build schema from snapshot if available (for snapshot mode)
      val df = snapshotMetadata match {
        case Some(metadata) =>
          // For snapshot mode, only include the PK field we need to read (not all user fields)
          // FFI reader will only read the columns specified in the schema
          val pkField = metadata.collection.schema.fields.find(_.getFieldIDAsLong == pkFieldId)
          val pkSchema = pkField match {
            case Some(field) =>
              // Create schema with only the PK field
              import org.apache.spark.sql.types._
              val pkFieldType = MilvusSnapshotReader.fieldToSparkType(field)
              StructType(Seq(StructField(field.name, pkFieldType, nullable = true)))
            case None =>
              // Fallback: use full schema if PK field not found
              logger.warn(s"PK field with ID $pkFieldId not found in snapshot schema, using full schema")
              MilvusSnapshotReader.toSparkSchema(metadata.collection.schema, includeSystemFields = false)
          }

          // Add extra columns for segment tracking
          val fullSchema = pkSchema
            .add("segment_id", org.apache.spark.sql.types.LongType, false)
            .add("row_offset", org.apache.spark.sql.types.LongType, false)

          logger.info(s"Reading with schema: ${fullSchema.fieldNames.mkString(", ")}")

          spark.read
            .schema(fullSchema)
            .format("milvus")
            .options(options)
            .load()

        case None =>
          // Client-based mode (existing behavior)
          spark.read
            .format("milvus")
            .options(options)
            .load()
      }

      // Validate that segment_id and row_offset are present
      if (!df.columns.contains("segment_id") || !df.columns.contains("row_offset")) {
        return Left(ConnectionError(
          message = "Failed to read collection data with segment_id and row_offset. " +
            "Ensure milvus.extra.columns is set correctly."
        ))
      }

      Right(df)
    } catch {
      case e: Exception =>
        logger.error(s"Failed to read Milvus collection ${config.collectionName}", e)
        Left(ConnectionError(
          message = s"Failed to read Milvus collection ${config.collectionName}: ${e.getMessage}",
          cause = Some(e)
        ))
    }
  }

  /**
   * Validate schema compatibility between original and new field data
   */
  private def validateSchemaCompatibility(
      originalDF: DataFrame,
      backfillDF: DataFrame,
      pkName: String
  ): Either[BackfillError, Unit] = {
    try {
      // Find the primary key field in original data
      val pkField = originalDF.schema.fields.find(_.name == pkName)
        .getOrElse {
          return Left(SchemaValidationError(
            s"Original collection data must have primary key field '$pkName'"
          ))
        }

      // Find the pk field in new field data
      val newPkField = backfillDF.schema.fields.find(_.name == "pk")
        .getOrElse {
          return Left(SchemaValidationError("New field data must have 'pk' field"))
        }

      // Validate types match
      if (pkField.dataType != newPkField.dataType) {
        return Left(SchemaValidationError(
          s"Primary key type mismatch: original=${pkField.dataType}, new=${newPkField.dataType}"
        ))
      }

      Right(())

    } catch {
      case e: Exception =>
        logger.error("Failed to validate schema compatibility", e)
        Left(SchemaValidationError(
          s"Failed to validate schema compatibility: ${e.getMessage}"
        ))
    }
  }

  /**
   * Perform left join between original and new field data
   */
  private def performJoin(
      originalDF: DataFrame,
      backfillDF: DataFrame,
      pkName: String
  ): DataFrame = {
    originalDF.join(backfillDF, originalDF(pkName) === backfillDF("pk"), "left")
  }

  /**
   * Retrieve Milvus metadata (collection ID and segment-to-partition mapping)
   * Supports multi-partition collections by tracking partition ID for each segment
   */
  private def retrieveMilvusMetadata(
      config: BackfillConfig,
      client: MilvusClient
  ): Either[BackfillError, (Long, Map[Long, Long])] = {
    try {
      val segments = client.getSegments(config.databaseName, config.collectionName)
        .getOrElse {
          return Left(ConnectionError(
            message = s"No segments found for collection ${config.collectionName}"
          ))
        }

      if (segments.isEmpty) {
        return Left(ConnectionError(
          message = s"Collection ${config.collectionName} has no segments"
        ))
      }

      val collectionID = segments.head.collectionID

      // Build mapping of segment ID -> partition ID to support multi-partition collections
      val segmentToPartitionMap = segments.map { seg =>
        seg.segmentID -> seg.partitionID
      }.toMap

      Right((collectionID, segmentToPartitionMap))

    } catch {
      case e: Exception =>
        logger.error(s"Failed to retrieve Milvus metadata for collection ${config.collectionName}", e)
        Left(ConnectionError(
          message = s"Failed to retrieve Milvus metadata: ${e.getMessage}",
          cause = Some(e)
        ))
    }
  }

  /**
   * Process each segment separately by distributing to Spark executors
   * Each segment is processed by exactly one FFI writer on a single executor
   * Supports multi-partition collections by tracking partition ID per segment
   */
  private def processSegments(
      spark: SparkSession,
      joinedDF: DataFrame,
      collectionID: Long,
      segmentToPartitionMap: Map[Long, Long],
      segmentBasePathMap: Map[Long, String],
      config: BackfillConfig,
      newFieldNames: Seq[String],
      fieldNameToId: Map[String, Long] = Map.empty
  ): Either[BackfillError, Map[Long, SegmentBackfillResult]] = {

    try {
      // Prepare data: select only needed columns and add segment_id for partitioning
      val preparedDF = joinedDF
        .select((Seq("segment_id", "row_offset") ++ newFieldNames).map(col): _*)

      // Get the schema for new fields only (without segment_id and row_offset)
      val targetSchema = org.apache.spark.sql.types.StructType(
        newFieldNames.map(fieldName =>
          preparedDF.schema.fields.find(_.name == fieldName).get
        )
      )

      val segmentIds = segmentToPartitionMap.keys.toArray
      val segmentPartitioner = new SegmentPartitioner(segmentIds)

      // Repartition using custom partitioner, then sort by row_offset within each partition
      // CRITICAL: .copy() is required because queryExecution.toRdd produces an iterator
      // that reuses the same UnsafeRow buffer. Without copy, keyBy/partitionBy's
      // ExternalSorter stores references to the same mutable buffer, causing all
      // rows to contain the last row's data.
      val repartitionedRDD = preparedDF.queryExecution.toRdd
        .map(_.copy())  // Materialize each row to avoid UnsafeRow reuse
        .keyBy(_.getLong(0))  // segment_id is at index 0
        .partitionBy(segmentPartitioner)
        .values
        .mapPartitions(iter => iter.toSeq.sortBy(_.getLong(1)).iterator)  // Sort by row_offset

      // Broadcast configuration to executors
      val broadcastConfig = spark.sparkContext.broadcast(config)
      val broadcastCollectionID = spark.sparkContext.broadcast(collectionID)
      val broadcastSegmentToPartitionMap = spark.sparkContext.broadcast(segmentToPartitionMap)
      val broadcastSegmentBasePathMap = spark.sparkContext.broadcast(segmentBasePathMap)
      val broadcastTargetSchema = spark.sparkContext.broadcast(targetSchema)
      val broadcastFieldNameToId = spark.sparkContext.broadcast(fieldNameToId)

      val results = repartitionedRDD.mapPartitions { iter =>
        if (!iter.hasNext) Iterator.empty
        else processSegmentPartition(
          iter,
          broadcastConfig.value,
          broadcastCollectionID.value,
          broadcastSegmentToPartitionMap.value,
          broadcastSegmentBasePathMap.value,
          broadcastTargetSchema.value,
          broadcastFieldNameToId.value
        )
      }.collect()

      // Cleanup broadcast variables
      broadcastConfig.unpersist()
      broadcastCollectionID.unpersist()
      broadcastSegmentToPartitionMap.unpersist()
      broadcastSegmentBasePathMap.unpersist()
      broadcastTargetSchema.unpersist()
      broadcastFieldNameToId.unpersist()

      // Check for failures
      val failures = results.filter(_._2.isDefined)
      if (failures.nonEmpty) {
        val firstFailure = failures.head
        val error = firstFailure._2.get
        return Left(WriteError(
          segmentId = firstFailure._1.segmentId,
          outputPath = firstFailure._1.outputPath,
          message = s"Failed to write ${failures.length} segment(s): ${error.getMessage}",
          cause = Some(error)
        ))
      }

      // Extract successful results
      val successfulResults = results.map { case (result, _) =>
        result.segmentId -> result
      }.toMap

      // Log summary statistics
      val totalTime = results.map(_._1.executionTimeMs).sum
      val avgTime = if (results.nonEmpty) totalTime / results.length else 0
      val totalRows = results.map(_._1.rowCount).sum

      logger.info("=== Backfill Summary ===")
      logger.info(s"Total segments: ${results.length}")
      logger.info(s"Total rows processed: $totalRows")
      logger.info(s"Total time for all segments: ${totalTime}ms")
      logger.info(s"Average time per segment: ${avgTime}ms")

      Right(successfulResults)

    } catch {
      case e: Exception =>
        logger.error("Failed to process segments", e)
        Left(SegmentProcessingError(
          segmentId = -1,
          message = s"Failed to process segments: ${e.getMessage}",
          cause = Some(e)
        ))
    }
  }

  /**
   * Process a single partition containing exactly one segment
   * This is called by each Spark executor to write one segment's data
   */
  private def processSegmentPartition(
      iter: Iterator[InternalRow],
      config: BackfillConfig,
      collectionID: Long,
      segmentToPartitionMap: Map[Long, Long],
      segmentBasePathMap: Map[Long, String],
      targetSchema: org.apache.spark.sql.types.StructType,
      fieldNameToId: Map[String, Long] = Map.empty
  ): Iterator[(SegmentBackfillResult, Option[Throwable])] = {

    val firstRow = iter.next()
    val segmentID = firstRow.getLong(0)
    val partitionID = segmentToPartitionMap(segmentID)
    val startTime = System.currentTimeMillis()

    // Create writer â€” use manifest basePath if available, otherwise generate path
    val writeOptions = segmentBasePathMap.get(segmentID) match {
      case Some(basePath) => config.getS3WriteOptionsForBasePath(basePath, segmentID, fieldNameToId)
      case None => config.getS3WriteOptions(collectionID, partitionID, segmentID, fieldNameToId)
    }
    val outputPath = writeOptions("milvus.writer.customPath")

    val optionsMap = new CaseInsensitiveStringMap(writeOptions.asJava)
    val batchWrite = new MilvusLoonBatchWrite(targetSchema, MilvusOption(optionsMap))
    val writer = batchWrite.createBatchWriterFactory(null).createWriter(0, System.currentTimeMillis())

    try {
      var rowCount = 0L
      var nullRowCount = 0L

      def writeRow(row: InternalRow): Unit = {
        val targetFields = (2 until row.numFields).map(i =>
          row.get(i, targetSchema.fields(i - 2).dataType)
        ).toArray
        if (targetFields.forall(_ == null)) nullRowCount += 1
        writer.write(new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(targetFields))
        rowCount += 1
      }

      writeRow(firstRow)
      iter.foreach(writeRow)

      val commitMessage = writer.commit()
      val (manifestPaths, committedVersion) = commitMessage match {
        case msg: MilvusLoonCommitMessage => (Seq(msg.manifestPath), msg.committedVersion)
        case _ => (Seq.empty[String], -1L)
      }

      batchWrite.commit(Array(commitMessage))
      writer.close()

      Iterator.single((SegmentBackfillResult(
        segmentId = segmentID,
        rowCount = rowCount,
        manifestPaths = manifestPaths,
        outputPath = outputPath,
        executionTimeMs = System.currentTimeMillis() - startTime,
        committedVersion = committedVersion
      ), None))

    } catch {
      case e: Exception =>
        writer.abort()
        writer.close()
        Iterator.single((SegmentBackfillResult(
          segmentId = segmentID,
          rowCount = 0,
          manifestPaths = Seq.empty,
          outputPath = outputPath,
          executionTimeMs = System.currentTimeMillis() - startTime
        ), Some(e)))
    }
  }

  /**
   * Load and parse snapshot metadata from file.
   * Returns None if snapshot path is empty or parsing fails.
   */
  private def loadSnapshotMetadata(
      spark: SparkSession,
      snapshotPath: String,
      config: BackfillConfig
  ): Option[SnapshotMetadata] = {
    if (snapshotPath == null || snapshotPath.isEmpty) return None

    readSnapshotJson(spark, snapshotPath, config) match {
      case Right(json) if json.nonEmpty =>
        MilvusSnapshotReader.parseSnapshotMetadata(json) match {
          case Right(metadata) => Some(metadata)
          case Left(e) =>
            logger.warn(s"Failed to parse snapshot metadata: ${e.getMessage}")
            None
        }
      case _ => None
    }
  }

  /**
   * Extract collection metadata from snapshot: collectionID, segment-to-partition mapping, segment base paths.
   * Partition IDs are derived from manifest basePaths: {rootPath}/insert_log/{col_id}/{part_id}/{seg_id}
   */
  private def extractMetadataFromSnapshot(
      metadata: SnapshotMetadata
  ): (Long, Map[Long, Long], Map[Long, String]) = {
    val collectionID = metadata.snapshotInfo.collectionId

    val manifestList = metadata.storageV2ManifestList.getOrElse(Seq.empty)
    var segmentToPartitionMap = Map.empty[Long, Long]
    var segmentBasePathMap = Map.empty[Long, String]

    for (item <- manifestList) {
      val segId = item.segmentID
      MilvusSnapshotReader.parseManifestContent(item.manifest) match {
        case Right(mc) =>
          segmentBasePathMap += (segId -> mc.basePath)
          // Extract partition ID from basePath: .../insert_log/{col_id}/{part_id}/{seg_id}
          val parts = mc.basePath.split("/")
          val insertLogIdx = parts.indexOf("insert_log")
          if (insertLogIdx >= 0 && insertLogIdx + 2 < parts.length) {
            try {
              val partitionId = parts(insertLogIdx + 2).toLong
              segmentToPartitionMap += (segId -> partitionId)
            } catch {
              case _: NumberFormatException =>
                logger.warn(s"Failed to parse partition ID from basePath: ${mc.basePath}")
            }
          }
        case Left(e) =>
          logger.warn(s"Failed to parse manifest for segment $segId: ${e.getMessage}")
      }
    }

    logger.info(s"Extracted from snapshot: collectionID=$collectionID, " +
      s"segments=${segmentBasePathMap.keys.mkString(",")}")

    (collectionID, segmentToPartitionMap, segmentBasePathMap)
  }

  /**
   * Write backfill result JSON to the given output path (S3 or local).
   * Uses Spark's Hadoop FileSystem API for portability.
   */
  def writeResultJson(spark: SparkSession, result: BackfillResult, outputPath: String): Unit = {
    try {
      val hadoopPath = new Path(outputPath)
      val fs = hadoopPath.getFileSystem(spark.sparkContext.hadoopConfiguration)
      val out = fs.create(hadoopPath, true)
      try {
        out.writeBytes(result.toJson)
      } finally {
        out.close()
      }
      logger.info(s"Backfill result JSON written to: $outputPath")
    } catch {
      case e: Exception =>
        logger.error(s"Failed to write result JSON to $outputPath: ${e.getMessage}", e)
    }
  }

  /**
   * Read snapshot JSON content from S3 or local file system.
   * Returns the JSON string.
   */
  private def readSnapshotJson(
      spark: SparkSession,
      snapshotPath: String,
      config: BackfillConfig
  ): Either[BackfillError, String] = {
    if (snapshotPath == null || snapshotPath.isEmpty) {
      return Right("") // Empty path means use client fallback
    }

    try {
      // Check if it's an S3 path
      if (snapshotPath.startsWith("s3://") || snapshotPath.startsWith("s3a://")) {

        // Construct full S3 path (ensure s3a:// scheme for Hadoop)
        val s3Path = if (snapshotPath.startsWith("s3://")) {
          snapshotPath.replace("s3://", "s3a://")
        } else {
          snapshotPath
        }

        // Configure S3 settings in Spark's Hadoop Configuration
        val hadoopConf = spark.sparkContext.hadoopConfiguration
        hadoopConf.set("fs.s3a.endpoint", config.s3Endpoint)
        hadoopConf.set("fs.s3a.access.key", config.s3AccessKey)
        hadoopConf.set("fs.s3a.secret.key", config.s3SecretKey)
        hadoopConf.set("fs.s3a.path.style.access", "true")
        hadoopConf.set("fs.s3a.connection.ssl.enabled", if (config.s3UseSSL) "true" else "false")

        // Use Spark's DataFrame API to read the file (avoids Hadoop version issues)
        val df = spark.read.text(s3Path)
        val json = df.collect().map(_.getString(0)).mkString("\n")

        Right(json)

      } else {
        // Local file path, read directly
        val source = scala.io.Source.fromFile(snapshotPath)
        try {
          val json = source.mkString
          Right(json)
        } finally {
          source.close()
        }
      }
    } catch {
      case e: Exception =>
        logger.error(s"Failed to read snapshot JSON: ${e.getMessage}", e)
        Left(DataReadError(snapshotPath, s"Failed to read snapshot file: ${e.getMessage}", Some(e)))
    }
  }

}
